{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de un Modelo de spaCy para Detección de Entidades Nombradas (NER)\n",
    "---\n",
    "Este notebook tiene como objetivo entrenar un modelo de procesamiento de lenguaje natural (NLP) utilizando la biblioteca spaCy para realizar tareas de detección de entidades nombradas (NER, por sus siglas en inglés).\n",
    "\n",
    "La detección de entidades nombradas es una técnica clave en NLP que permite identificar y clasificar entidades específicas en un texto, como nombres de personas, organizaciones, ubicaciones, fechas, cantidades, entre otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos del Notebook\n",
    "\n",
    "1. **Preparar los datos de entrenamiento**: Generar un conjunto de datos etiquetados para el entrenamiento del modelo.\n",
    "2. **Configurar el modelo**: Definir los parámetros del modelo y configurar spaCy para trabajar con datos personalizados.\n",
    "3. **Entrenar el modelo**: Utilizar los datos para ajustar el modelo de spaCy y mejorar su capacidad para detectar las entidades deseadas.\n",
    "4. **Evaluar el modelo**: Validar el rendimiento del modelo utilizando métricas como precisión, exhaustividad y F1.\n",
    "5. **Guardar y reutilizar el modelo**: Exportar el modelo entrenado para integrarlo en otras aplicaciones.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Importación de librerías__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "from pathlib import Path\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy.scorer import Scorer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparar los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Obtener ejemplos de calles\n",
    "\n",
    "Para crear nuestro set de entrenamiento, utilizaremos las calles de la ciudad de Madrid. Así, crearemos ejemplos con nombres reales que más tarde ayudarán a generalizar al modelo.\n",
    "\n",
    "Descargamos los datos del callejero de Madrid desde [aquí](https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=b3c41f3cf6a6c410VgnVCM2000000c205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_callejero = pd.read_csv('../data/VialesVigentes_20241226.csv', sep = ';', encoding = 'cp1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargados los datos, los visualizamos para entender con qué información estamos trabajando. Nos encontramos con 15 columnas, de las cuales solo nos interesan 3: VIA_CLASE, VIA_PAR y VIA_NOMBRE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COD_VIA</th>\n",
       "      <th>VIA_CLASE</th>\n",
       "      <th>VIA_PAR</th>\n",
       "      <th>VIA_NOMBRE</th>\n",
       "      <th>VIA_NOMBRE_ACENTOS</th>\n",
       "      <th>COD_VIA_COMIENZA</th>\n",
       "      <th>CLASE_COMIENZA</th>\n",
       "      <th>PARTICULA_COMIENZA</th>\n",
       "      <th>NOMBRE_COMIENZA</th>\n",
       "      <th>NOMBRE_ACENTOS_COMIENZA</th>\n",
       "      <th>COD_VIA_TERMINA</th>\n",
       "      <th>CLASE_TERMINA</th>\n",
       "      <th>PARTICULA_TERMINA</th>\n",
       "      <th>NOMBRE_TERMINA</th>\n",
       "      <th>NOMBRE_ACENTOS_TERMINA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31001337</td>\n",
       "      <td>AUTOVÍA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A-1</td>\n",
       "      <td>A-1</td>\n",
       "      <td>31001349</td>\n",
       "      <td>AUTOVÍA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M-30</td>\n",
       "      <td>M-30</td>\n",
       "      <td>99000003</td>\n",
       "      <td>LUGAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LIMITE TERMINO MUNICIPAL</td>\n",
       "      <td>LÍMITE TÉRMINO MUNICIPAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31001336</td>\n",
       "      <td>AUTOVÍA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A-2</td>\n",
       "      <td>A-2</td>\n",
       "      <td>310200</td>\n",
       "      <td>CALLE</td>\n",
       "      <td>DE</td>\n",
       "      <td>FRANCISCO SILVELA</td>\n",
       "      <td>FRANCISCO SILVELA</td>\n",
       "      <td>99000003</td>\n",
       "      <td>LUGAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LIMITE TERMINO MUNICIPAL</td>\n",
       "      <td>LÍMITE TÉRMINO MUNICIPAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31001342</td>\n",
       "      <td>AUTOVÍA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A-3</td>\n",
       "      <td>A-3</td>\n",
       "      <td>480800</td>\n",
       "      <td>PLAZA</td>\n",
       "      <td>DE</td>\n",
       "      <td>MARIANO DE CAVIA</td>\n",
       "      <td>MARIANO DE CAVIA</td>\n",
       "      <td>99000003</td>\n",
       "      <td>LUGAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LIMITE TERMINO MUNICIPAL</td>\n",
       "      <td>LÍMITE TÉRMINO MUNICIPAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31001334</td>\n",
       "      <td>AUTOVÍA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A-4</td>\n",
       "      <td>A-4</td>\n",
       "      <td>31001349</td>\n",
       "      <td>AUTOVÍA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M-30</td>\n",
       "      <td>M-30</td>\n",
       "      <td>99000003</td>\n",
       "      <td>LUGAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LIMITE TERMINO MUNICIPAL</td>\n",
       "      <td>LÍMITE TÉRMINO MUNICIPAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31001341</td>\n",
       "      <td>AUTOVÍA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A-42</td>\n",
       "      <td>A-42</td>\n",
       "      <td>468400</td>\n",
       "      <td>AVENIDA</td>\n",
       "      <td>DEL</td>\n",
       "      <td>MANZANARES</td>\n",
       "      <td>MANZANARES</td>\n",
       "      <td>99000003</td>\n",
       "      <td>LUGAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LIMITE TERMINO MUNICIPAL</td>\n",
       "      <td>LÍMITE TÉRMINO MUNICIPAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    COD_VIA VIA_CLASE VIA_PAR VIA_NOMBRE VIA_NOMBRE_ACENTOS  COD_VIA_COMIENZA  \\\n",
       "0  31001337   AUTOVÍA     NaN        A-1                A-1          31001349   \n",
       "1  31001336   AUTOVÍA     NaN        A-2                A-2            310200   \n",
       "2  31001342   AUTOVÍA     NaN        A-3                A-3            480800   \n",
       "3  31001334   AUTOVÍA     NaN        A-4                A-4          31001349   \n",
       "4  31001341   AUTOVÍA     NaN       A-42               A-42            468400   \n",
       "\n",
       "  CLASE_COMIENZA PARTICULA_COMIENZA    NOMBRE_COMIENZA  \\\n",
       "0        AUTOVÍA                NaN               M-30   \n",
       "1          CALLE                 DE  FRANCISCO SILVELA   \n",
       "2          PLAZA                 DE   MARIANO DE CAVIA   \n",
       "3        AUTOVÍA                NaN               M-30   \n",
       "4        AVENIDA                DEL         MANZANARES   \n",
       "\n",
       "  NOMBRE_ACENTOS_COMIENZA  COD_VIA_TERMINA CLASE_TERMINA PARTICULA_TERMINA  \\\n",
       "0                    M-30         99000003         LUGAR               NaN   \n",
       "1       FRANCISCO SILVELA         99000003         LUGAR               NaN   \n",
       "2        MARIANO DE CAVIA         99000003         LUGAR               NaN   \n",
       "3                    M-30         99000003         LUGAR               NaN   \n",
       "4              MANZANARES         99000003         LUGAR               NaN   \n",
       "\n",
       "             NOMBRE_TERMINA    NOMBRE_ACENTOS_TERMINA  \n",
       "0  LIMITE TERMINO MUNICIPAL  LÍMITE TÉRMINO MUNICIPAL  \n",
       "1  LIMITE TERMINO MUNICIPAL  LÍMITE TÉRMINO MUNICIPAL  \n",
       "2  LIMITE TERMINO MUNICIPAL  LÍMITE TÉRMINO MUNICIPAL  \n",
       "3  LIMITE TERMINO MUNICIPAL  LÍMITE TÉRMINO MUNICIPAL  \n",
       "4  LIMITE TERMINO MUNICIPAL  LÍMITE TÉRMINO MUNICIPAL  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_callejero.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez seleccionadas las columnas de interés, crearemos una columna adicional que sea la concatenación de la partícula y el nombre de la vía, ya que no estamos interesados de que nuestro modelo detecte las partículas de forma distinta al nombre de la vía.\n",
    "\n",
    "Así mismo, eliminaremos los registros que contengan vacíos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calles = df_callejero[df_callejero.VIA_CLASE != 'AUTOVÍA'][['VIA_CLASE', 'VIA_PAR', 'VIA_NOMBRE']]\n",
    "\n",
    "df_calles['NOMBRE_VIA'] = df_calles['VIA_PAR'] + ' ' + df_calles['VIA_NOMBRE']\n",
    "df_calles.drop(columns = ['VIA_PAR', 'VIA_NOMBRE'], inplace  = True)\n",
    "\n",
    "df_calles.rename(columns={'VIA_CLASE': 'TIPO_VIA'}, inplace = True)\n",
    "df_calles.dropna(inplace =  True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es una tabla con dos columnas, el tipo de vía y el nombre de la vía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIPO_VIA</th>\n",
       "      <th>NOMBRE_VIA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CALLE</td>\n",
       "      <td>DEL ABAD JUAN CATALAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CALLE</td>\n",
       "      <td>DE LA ABADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CALLE</td>\n",
       "      <td>DE LOS ABADES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CALLE</td>\n",
       "      <td>DE LA ABADESA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CALLE</td>\n",
       "      <td>DE ABALOS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TIPO_VIA             NOMBRE_VIA\n",
       "7     CALLE  DEL ABAD JUAN CATALAN\n",
       "8     CALLE            DE LA ABADA\n",
       "9     CALLE          DE LOS ABADES\n",
       "10    CALLE          DE LA ABADESA\n",
       "11    CALLE              DE ABALOS"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_calles.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Generar direcciones ficticias\n",
    "\n",
    "Para terminar de crear las direcciones ficticias, añadiremos más señas a las calles de Madrid. Para ello, de manera aleatoria, incluiremos información sobre el número y la combinación piso-letra. Este paso lo podemos ir mejorando a medida que descubramos casos reales que no están contemplados en estos que generamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_datos_entrenamiento(df, n):\n",
    "    \"\"\"\n",
    "    Genera N líneas aleatorias a partir de un DataFrame base.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame base para seleccionar líneas.\n",
    "        n (int): Número de líneas aleatorias a generar.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con N líneas aleatorias generadas.\n",
    "    \"\"\"\n",
    "    lineas = []\n",
    "    for _ in range(n):\n",
    "        # Seleccionar una línea aleatoria del DataFrame base\n",
    "        row = df.sample(1).copy()\n",
    "\n",
    "        # Agregar un número aleatorio\n",
    "        pre_numero = random.choice(['n*', 'nr', 'nº', '', 'n'])\n",
    "        numero = random.randint(1, 50)\n",
    "        post_numero = random.choice(['', '', '', ','])\n",
    "        row['NUMERO'] = f'{pre_numero}{numero}{post_numero}'\n",
    "\n",
    "        # Agregar combinación aleatoria de piso y letra\n",
    "        piso = random.randint(1, 10)\n",
    "        letra = random.choice(['A', 'B', 'C', 'D', 'IZDA', 'DCHA', 'CTRO'])\n",
    "        espacio = random.choice([' ', '*', 'º', '', '-'])\n",
    "        combinacion = f\"{piso}{espacio}{letra}\"\n",
    "        row['RESTO'] = combinacion\n",
    "\n",
    "        # Agregar la línea generada a la lista\n",
    "        lineas.append(row)\n",
    "\n",
    "    # Combinar todas las líneas en un nuevo DataFrame\n",
    "    return pd.concat(lineas, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = generar_datos_entrenamiento(df_calles, 7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Estructurar los datos para el modelo\n",
    "\n",
    "Una vez terminamos de generar las direcciones ficticias, daremos forma a nuestros datos para poder entrenar el modelo de spacy. Esta estructura es específica para la detección de entidades con la librería SpaCy.\n",
    "\n",
    "Se trata de un tuple en el que encontraremos el texto completo en la primera posición y la definición de las entidades en la segunda. Aquí un ejemplo:\n",
    "\n",
    "```\n",
    "('CALLE DE LOS MARTIRES DE PARACUELLOS n44 3 D',\n",
    "{'entities': [(0, 5, 'TIPO_VIA'),\n",
    "    (6, 36, 'NOMBRE_VIA'),\n",
    "    (37, 40, 'NUMERO'),\n",
    "    (41, 44, 'RESTO')]})\n",
    "```\n",
    "\n",
    "En nuestro caso el modelo tendrá 4 tipo de entidades: el tipo de vía, el nombre de la vía, el número y el resto de la dirección. Esto provoca que todo el texto esté identificado con alguna entidad. \n",
    "\n",
    "Sin embargo, también podríamos crear modelos que detecten, por ejemplo, únicamente el nombre de la vía. De esta manera no todo el texto estaría categorizado, sino solo una parte.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_entidades(row):\n",
    "    texto = row['texto']\n",
    "    entidades = []\n",
    "    start = 0\n",
    "    for col, label in zip(['TIPO_VIA', 'NOMBRE_VIA', 'NUMERO', 'RESTO'], ['TIPO_VIA', 'NOMBRE_VIA', 'NUMERO', 'RESTO']):\n",
    "        value = str(row[col])\n",
    "        start = texto.find(value, start)  # Buscar el índice inicial de la entidad\n",
    "        if start != -1:\n",
    "            end = start + len(value)  # Índice final de la entidad\n",
    "            entidades.append((start, end, label))\n",
    "            start = end  # Actualizar el inicio para evitar errores en textos repetidos\n",
    "    return (texto, {'entities': entidades})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['texto'] = train_set['TIPO_VIA'] + ' ' + train_set['NOMBRE_VIA'] + ' ' + train_set['NUMERO'].astype(str) + ' ' + train_set['RESTO']\n",
    "train_set['entidades'] = train_set.apply(crear_entidades, axis=1)\n",
    "\n",
    "lista = train_set['entidades'].to_list()\n",
    "\n",
    "train, test = train_test_split(lista, test_size=0.25, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurar el modelo\n",
    "\n",
    "Para entrenar el modelo, primeramente necesitamos seleccionar qué componentes del pipeline del modelo pre-configurado queremos modificar con nuestros datos de entrenamiento. En nuestro caso serán: ner, trf_wordpiecer y trf_tok2vec. \n",
    "\n",
    "Los dos últimos son componentes que se encargan de la tokenización y la creación de vectores. Su modificación ayudará al componente ner a mejorar su capacidad de generalización.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "ner = nlp.get_pipe('ner')\n",
    "\n",
    "for _,annotations in train:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "pipe_exceptions = ['ner', 'trf_wordpiecer', 'trf_tok2vec']\n",
    "unaffected_pipe = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenar el modelo\n",
    "\n",
    "Para entrenar el modelo, deshabilitaremos los componentes del pipeline que previamente hemos indicado que no queremos modificar. Después creamos un bucle de entrenamiento con sus respectivos batches (lotes), en el que actualizaremos el modelo y que calcularemos los losses a cada paso del batch. Realizaremos esto tantas veces como iteraciones indiquemos.\n",
    "\n",
    "Cabe destacar el parámetro \"drop\", que utilizaremos para descartar ejemplos de entrenamiento y así evitar el overfitting (sobreajuste)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Losses: {'ner': np.float32(2569.1147)}\n",
      "Iteration 1 - Losses: {'ner': np.float32(6.327425)}\n",
      "Iteration 2 - Losses: {'ner': np.float32(3.6257806)}\n",
      "Iteration 3 - Losses: {'ner': np.float32(4.602195)}\n",
      "Iteration 4 - Losses: {'ner': np.float32(13.887782)}\n",
      "Iteration 5 - Losses: {'ner': np.float32(4.488487)}\n",
      "Iteration 6 - Losses: {'ner': np.float32(5.294967)}\n",
      "Iteration 7 - Losses: {'ner': np.float32(9.12825)}\n",
      "Iteration 8 - Losses: {'ner': np.float32(13.653158)}\n",
      "Iteration 9 - Losses: {'ner': np.float32(1.9419212)}\n",
      "Iteration 10 - Losses: {'ner': np.float32(1.6027925e-05)}\n",
      "Iteration 11 - Losses: {'ner': np.float32(7.8644604e-07)}\n",
      "Iteration 12 - Losses: {'ner': np.float32(2.0304881e-07)}\n",
      "Iteration 13 - Losses: {'ner': np.float32(3.8328807e-09)}\n",
      "Iteration 14 - Losses: {'ner': np.float32(3.9463973e-07)}\n",
      "Iteration 15 - Losses: {'ner': np.float32(24.40153)}\n",
      "Iteration 16 - Losses: {'ner': np.float32(2.0032232)}\n",
      "Iteration 17 - Losses: {'ner': np.float32(0.34888506)}\n",
      "Iteration 18 - Losses: {'ner': np.float32(26.6079)}\n",
      "Iteration 19 - Losses: {'ner': np.float32(10.627683)}\n",
      "Iteration 20 - Losses: {'ner': np.float32(3.449866e-07)}\n",
      "Iteration 21 - Losses: {'ner': np.float32(6.379792)}\n",
      "Iteration 22 - Losses: {'ner': np.float32(1.760126)}\n",
      "Iteration 23 - Losses: {'ner': np.float32(3.8275703e-09)}\n",
      "Iteration 24 - Losses: {'ner': np.float32(5.1639995e-09)}\n",
      "Iteration 25 - Losses: {'ner': np.float32(2.5164588e-06)}\n",
      "Iteration 26 - Losses: {'ner': np.float32(2.1348441e-10)}\n",
      "Iteration 27 - Losses: {'ner': np.float32(9.3136e-06)}\n",
      "Iteration 28 - Losses: {'ner': np.float32(7.74914e-08)}\n",
      "Iteration 29 - Losses: {'ner': np.float32(1.7348357e-08)}\n",
      "Iteration 30 - Losses: {'ner': np.float32(5.6120466e-11)}\n",
      "Iteration 31 - Losses: {'ner': np.float32(1.3682483e-09)}\n",
      "Iteration 32 - Losses: {'ner': np.float32(2.0602074e-07)}\n",
      "Iteration 33 - Losses: {'ner': np.float32(1.9053644e-08)}\n",
      "Iteration 34 - Losses: {'ner': np.float32(1.3200597e-12)}\n",
      "Iteration 35 - Losses: {'ner': np.float32(5.8612577e-12)}\n",
      "Iteration 36 - Losses: {'ner': np.float32(4.627638e-10)}\n",
      "Iteration 37 - Losses: {'ner': np.float32(4.518575e-12)}\n",
      "Iteration 38 - Losses: {'ner': np.float32(3.5550846e-13)}\n",
      "Iteration 39 - Losses: {'ner': np.float32(4.7218895e-10)}\n",
      "Iteration 40 - Losses: {'ner': np.float32(2.0167928e-10)}\n",
      "Iteration 41 - Losses: {'ner': np.float32(2.2642351e-13)}\n",
      "Iteration 42 - Losses: {'ner': np.float32(2.5667179e-12)}\n",
      "Iteration 43 - Losses: {'ner': np.float32(1.1695987e-12)}\n",
      "Iteration 44 - Losses: {'ner': np.float32(8.289659e-12)}\n",
      "Iteration 45 - Losses: {'ner': np.float32(8.5439246e-13)}\n",
      "Iteration 46 - Losses: {'ner': np.float32(1.32607154e-11)}\n",
      "Iteration 47 - Losses: {'ner': np.float32(9.618794e-13)}\n",
      "Iteration 48 - Losses: {'ner': np.float32(1.9447209e-14)}\n",
      "Iteration 49 - Losses: {'ner': np.float32(18.760252)}\n"
     ]
    }
   ],
   "source": [
    "nr_iter = 50\n",
    "\n",
    "with nlp.disable_pipes(*unaffected_pipe):\n",
    "    for iteration in range(nr_iter):\n",
    "        random.shuffle(train)\n",
    "        losses = {}\n",
    "\n",
    "        batches = minibatch(train, size=8)\n",
    "        for batch in batches:\n",
    "            example = []\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example.append(Example.from_dict(doc, annotations))\n",
    "\n",
    "            nlp.update(example, drop = 0.3, losses=losses)\n",
    "\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration} - Losses: {losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('../models/address_ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluar el modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, evaluaremos el rendimiento del modelo entrenado utilizando el conjunto de datos de prueba. La evaluación se realizará mediante métricas estándar como precisión, exhaustividad y F1-score. Estas métricas nos permitirán entender cómo el modelo es capaz de identificar y clasificar las entidades nombradas en textos no vistos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TIPO_VIA': {'p': 0.9988571428571429,\n",
       "  'r': 0.9988571428571429,\n",
       "  'f': 0.9988571428571429},\n",
       " 'NOMBRE_VIA': {'p': 0.9942954934398175, 'r': 0.996, 'f': 0.9951470168427062},\n",
       " 'NUMERO': {'p': 0.9988584474885844, 'r': 1.0, 'f': 0.9994288977727013},\n",
       " 'RESTO': {'p': 0.9994285714285714,\n",
       "  'r': 0.9994285714285714,\n",
       "  'f': 0.9994285714285714}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = []\n",
    "scorer = Scorer()\n",
    "for text, annotations in test:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    example.predicted = nlp(str(example.predicted))\n",
    "    examples.append(example)\n",
    "\n",
    "results_scorer = scorer.score(examples)\n",
    "results_scorer['ents_per_type']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "address",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
